{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVlQCcUDOmRGHeBikSZHe+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nyubk/PP/blob/main/Red_Neuronal_Multicapa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=https://edu.rcastellanos.cdmx.gob.mx/sitio_lad/images/Imagotipo_compacto_color_600px.png width= 300>\n",
        "\n",
        "#La influencia de la salud mental en la deserción escolar\n",
        "\n",
        "##*Factores de potencialización en la deserción escolar*\n",
        "### Entrenamiento de la Red Neuronal Multicapa utilizando la información obtenida del ETL al *dataset* 9_ENAH_students.csv\n",
        "\n",
        "  Cruz, R (2023): Factores de la deserción escolar. v1.0. ENAH - Dep. Antropología Física. *dataset*. https://www.kaggle.com/competitions/etl-datasets-salud-mental/data?select=9_ENAH_students.csv\n",
        "\n",
        "*Equipo 2*\n",
        "\n",
        "Integrantes:\n",
        "\n",
        "Espinosa Flores America Daniela\n",
        "\n",
        "Rincon Ramírez Victor Francisco"
      ],
      "metadata": {
        "id": "rpzLdduphBBa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWTHhvTug812"
      },
      "outputs": [],
      "source": [
        "# Datos de entrada y salida\n",
        "X = np.array([[1, 1, 1], [1, 1, 0], [1, 0, 1], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 0, 1], [0, 0, 0]])\n",
        "Y = np.array([[1], [0], [1], [0], [1], [0], [1], [0]])\n",
        "\n",
        "# Definir la arquitectura de la red neuronal\n",
        "np.random.seed(int(np.sqrt(2**31)))\n",
        "input_neurons = 3\n",
        "hidden_neurons = 18\n",
        "output_neurons = 1\n",
        "\n",
        "# Inicialización de pesos\n",
        "weights_input_hidden = np.random.uniform(size=(input_neurons, hidden_neurons))\n",
        "weights_hidden_output = np.random.uniform(size=(hidden_neurons, output_neurons))\n",
        "\n",
        "# Inicialización de sesgos\n",
        "bias_hidden = np.random.uniform(size=(1, hidden_neurons))\n",
        "bias_output = np.random.uniform(size=(1, output_neurons))\n",
        "\n",
        "# Función de activación ReLU\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Derivada de la función ReLU\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "# Función de activación Sigmoid (para la capa de salida)\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Derivada de la función Sigmoid\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Inicialización de variables\n",
        "epochs = 10000\n",
        "learning_rate = 0.003\n",
        "\n",
        "# Lista para almacenar los valores de la función de pérdida (MSE)\n",
        "loss_values = []\n",
        "\n",
        "# Entrenamiento de la red neuronal\n",
        "for epoch in range(epochs):\n",
        "    # Paso hacia adelante (Forward pass)\n",
        "    input_layer = X\n",
        "\n",
        "    hidden_layer_input = np.dot(input_layer, weights_input_hidden) + bias_hidden\n",
        "    hidden_layer_output = relu(hidden_layer_input)\n",
        "\n",
        "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
        "    output = sigmoid(output_layer_input)\n",
        "\n",
        "    # Cálculo del error\n",
        "    error = Y - output\n",
        "    mse = np.mean((error)**2)\n",
        "    loss_values.append(mse)  # Almacenar el valor de la pérdida\n",
        "\n",
        "    # Mostrar el progreso\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        _mse = '{:.20f}'.format(mse)\n",
        "        print(f'Época {epoch+1}, Loss function (MSE): {_mse}')\n",
        "\n",
        "    # Retropropagación (Backpropagation)\n",
        "    d_output = error * sigmoid_derivative(output)\n",
        "    error_hidden = d_output.dot(weights_hidden_output.T)\n",
        "    d_hidden = error_hidden * relu_derivative(hidden_layer_output)\n",
        "\n",
        "    # Actualización de pesos\n",
        "    weights_hidden_output += hidden_layer_output.T.dot(d_output) * learning_rate\n",
        "    weights_input_hidden += input_layer.T.dot(d_hidden) * learning_rate\n",
        "    bias_output += np.sum(d_output, axis=0, keepdims=True) * learning_rate\n",
        "    bias_hidden += np.sum(d_hidden, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "# Paso hacia adelante final para obtener las predicciones\n",
        "input_layer = X\n",
        "hidden_layer_input = np.dot(input_layer, weights_input_hidden) + bias_hidden\n",
        "hidden_layer_output = relu(hidden_layer_input)\n",
        "output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
        "predictions = sigmoid(output_layer_input)\n",
        "\n",
        "print(\"Resultados finales:\")\n",
        "for i in range(len(X)):\n",
        "    print(f\"Input: {X[i]}, Target: {Y[i]}, Predictions {predictions[i]}\")\n",
        "\n",
        "# Graficar la función de pérdida\n",
        "plt.plot(loss_values)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.title('Loss Function Over Epochs')\n",
        "plt.show()\n",
        "\n",
        "# Función para predecir la salida para una nueva entrada\n",
        "def predict(X_new):\n",
        "    hidden_layer_input = np.dot(X_new, weights_input_hidden) + bias_hidden\n",
        "    hidden_layer_output = relu(hidden_layer_input)\n",
        "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
        "    predictions = sigmoid(output_layer_input)\n",
        "    return predictions"
      ]
    }
  ]
}